{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e0bf1f-7be4-4e3a-aaa7-2012c440ecd1",
   "metadata": {},
   "source": [
    "# 1b Nucleoid Mapping to Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "583601ce-b26f-4315-8d01-4134dc36aa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17135/2419956342.py:8: DeprecationWarning: The current Dask DataFrame implementation is deprecated. \n",
      "In a future release, Dask DataFrame will use new implementation that\n",
      "contains several improvements including a logical query planning.\n",
      "The user-facing DataFrame API will remain unchanged.\n",
      "\n",
      "The new implementation is already available and can be enabled by\n",
      "installing the dask-expr library:\n",
      "\n",
      "    $ pip install dask-expr\n",
      "\n",
      "and turning the query planning option on:\n",
      "\n",
      "    >>> import dask\n",
      "    >>> dask.config.set({'dataframe.query-planning': True})\n",
      "    >>> import dask.dataframe as dd\n",
      "\n",
      "API documentation for the new implementation is available at\n",
      "https://docs.dask.org/en/stable/dask-expr-api.html\n",
      "\n",
      "Any feedback can be reported on the Dask issue tracker\n",
      "https://github.com/dask/dask/issues \n",
      "\n",
      "  import dask.dataframe as dd\n"
     ]
    }
   ],
   "source": [
    "import trenchripper.trenchripper as tr\n",
    "\n",
    "import warnings\n",
    "import dask\n",
    "\n",
    "import numpy as np\n",
    "import skimage as sk\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask.distributed import wait\n",
    "\n",
    "warnings.filterwarnings(action=\"once\")\n",
    "\n",
    "# addition of active memory manager\n",
    "dask.config.set({'distributed.scheduler.active-memory-manager.start': True});\n",
    "dask.config.set({'distributed.scheduler.worker-ttl': \"5m\"});\n",
    "dask.config.set({'distributed.scheduler.allowed-failures': 100});\n",
    "\n",
    "dask_wd = \"/home/de64/scratch/de64/dask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e659081-0fa9-4e7f-a2df-e9b171c7cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50m\n",
      "1:00:00\n"
     ]
    }
   ],
   "source": [
    "dask_controller = tr.trcluster.dask_controller(\n",
    "    walltime=\"1:00:00\",\n",
    "    local=False,\n",
    "    n_workers=20,\n",
    "    n_workers_min=20,\n",
    "    memory=\"10GB\",\n",
    "    working_directory=dask_wd,\n",
    ")\n",
    "dask_controller.startdask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25918d59-cfac-4cc2-b08b-412c3cc3fd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://10.120.16.108:8787/status\">Dashboard</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dask_controller.displaydashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2b0f5-ee34-485a-b5aa-4926c9d1e99b",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d3ab8d6-ff8f-4f26-aa92-a3c3afec61e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_nucleoids_to_cells_df(nuc_df, cell_df):\n",
    "    \n",
    "    nuc_df_output = nuc_df\n",
    "    nuc_df = nuc_df.reset_index().loc[:, ('File Trench Index', 'timepoints','Objectid', 'centroid_x', 'centroid_y', 'File Parquet Index')].set_index(['File Trench Index','timepoints', 'Objectid'])\n",
    "    cell_df = cell_df.reset_index().loc[:,('File Trench Index', 'timepoints', 'Segment Index' , 'bbox_min_row', 'bbox_min_col', 'bbox_max_row', 'bbox_max_col', 'File Parquet Index')].set_index(['File Trench Index', \"timepoints\", \"Segment Index\"])\n",
    "\n",
    "    nuc_array = nuc_df.to_xarray().to_array().to_numpy()\n",
    "    nuc_array = np.moveaxis(nuc_array, 0, -1)\n",
    "    \n",
    "    cell_array = cell_df.to_xarray().to_array().to_numpy()\n",
    "    cell_array = np.moveaxis(cell_array, 0, -1)\n",
    "\n",
    "    del nuc_df, cell_df\n",
    "    \n",
    "    nuc_df_output[\"Segment Index\"], nuc_df_output[\"Cells Per Nucleoid\"], nuc_df_output[\"Nuc Parquet Index\"], nuc_df_output[\"Cell Parquet Index\"] = assign_nucleoids_to_cells(nuc_array = nuc_array, cell_array = cell_array)\n",
    "    \n",
    "    return nuc_df_output\n",
    "    \n",
    "#     mapping, cells_per_nuc, nuc_parquet, cells_parquet = assign_nucleoids_to_cells(nuc_array = nuc_array, cell_array = cell_array)\n",
    "    \n",
    "#     del nuc_array, cell_array\n",
    "#     nuc_df_output[\"Segment Index\"] = mapping\n",
    "#     nuc_df_output[\"Cells Per Nucleoid\"] = cells_per_nuc\n",
    "#     nuc_df_output[\"Nuc Parquet Index\"] = nuc_parquet\n",
    "#     nuc_df_output[\"Cell Parquet Index\"] = cells_parquet\n",
    "#     return nuc_df_output\n",
    "\n",
    "def assign_nucleoids_to_cells(nuc_array, cell_array):\n",
    "    \"\"\"\n",
    "    nuc_array and cell_array are numpy arrays of dimensions:\n",
    "    (# Trench ID indices x # Timepoints x Max number of cells per timepoint x Max number of nucleoids per timepoint)\n",
    "    \"\"\"\n",
    "    \n",
    "    # These ones have dimensions trench_idx X timepoints X  Max # nucleoids per image X 1\n",
    "    x = nuc_array[:,:,:,0][:,:,:,None] # x coordinate of nucleoid centroid\n",
    "    y = nuc_array[:,:,:,1][:,:,:,None] # y coordiate of nucleoid centroid\n",
    "\n",
    "    # These ones have dimensions trench_idx X timepoints X  1 X Max # of cells per image\n",
    "    min_row = cell_array[:,:,:,0][:,:,None,:]\n",
    "    min_col = cell_array[:,:,:,1][:,:,None,:]\n",
    "    max_row = cell_array[:,:,:,2][:,:,None,:]\n",
    "    max_col = cell_array[:,:,:,3][:,:,None,:]\n",
    "\n",
    "    # It compares all nucleoids with all cells\n",
    "    # Is x greater than min_column?\n",
    "    x_g_min_col = x > min_col\n",
    "    # Is x less than max_colum?n\n",
    "    x_l_max_col = x < max_col\n",
    "    # Is y greater than min_row?\n",
    "    y_g_min_row = y > min_row\n",
    "    y_l_max_row = y < max_row\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Each element is Trench id, timepoint, nucleoid id, cell id of nucleoid - cell pair that was true on the previous array\n",
    "    It will match the length of nuc_df if there are no multiple dfs.\n",
    "\n",
    "    - This will have trouble if a nucleoid is mapped 0 or >1 times.\n",
    "    '''\n",
    "    inside = x_g_min_col & x_l_max_col & y_g_min_row & y_l_max_row\n",
    "    \n",
    "    '''\n",
    "    2022-10-26\n",
    "    '''\n",
    "    # Finds the number of cells per nucleoid\n",
    "    n_cells_per_nuc_collapsed = np.sum(inside, axis=3)\n",
    "\n",
    "    # Get nans\n",
    "    not_nan = ~(np.isnan(x) | np.isnan(min_row))\n",
    "    not_nan_collapsed = np.any(not_nan,axis=3)\n",
    "\n",
    "    # Add a column of true to the end of the \"inside\" array\n",
    "    inside2 = np.append(inside, \n",
    "                        np.ones((inside.shape[0], inside.shape[1], inside.shape[2], 1), dtype=bool), axis=3)\n",
    "    first_cell = np.argmax(inside2, axis=3)\n",
    "\n",
    "    # Selects nucleoids that have no cells but are not nan, or have one or more cells\n",
    "    nuc_has_zero_cells = first_cell == inside.shape[3]\n",
    "    nuc_has_zero_cells_but_not_nan = (nuc_has_zero_cells) & (not_nan_collapsed)\n",
    "    nuc_has_one_or_more_cells = n_cells_per_nuc_collapsed > 0\n",
    "\n",
    "    # ONLY NO MAPPED NUCLEOIDS: Modify first cell to include -1 if the nucleoid was not mapped to any real cell\n",
    "    # This will happen if the nucleoid is mapped to cell inside.shape[3]\n",
    "    first_cell[nuc_has_zero_cells] = -1\n",
    "\n",
    "    # Boolean array (trench id X timepoint X nucleoid ): True if nucleoid is selected as valid\n",
    "    nuc_selected = nuc_has_zero_cells_but_not_nan | nuc_has_one_or_more_cells\n",
    "\n",
    "    # Recover parquet index of selected nucleoids and cells\n",
    "    nuc_selected_idx = np.nonzero(nuc_selected)\n",
    "\n",
    "    '''\n",
    "    GENERATE THE LISTS\n",
    "    '''\n",
    "    # Nucleoid parquet indices\n",
    "    nuc_parquet_ids = nuc_array[nuc_selected_idx[0:3]][:,2].astype('int64')\n",
    "\n",
    "    # Cell parquet indices\n",
    "    # It will be nan if nucleoid is not mapped to cell\n",
    "    cell_parquet_ids = cell_array[nuc_selected_idx[0],\n",
    "                                  nuc_selected_idx[1],\n",
    "                                  first_cell[nuc_selected_idx]][:,4].astype('int64')\n",
    "\n",
    "    # Number of cells per nucleoid\n",
    "    # It will be zero if nucleoid is not mapped to cell\n",
    "    cells_per_nuc = n_cells_per_nuc_collapsed[nuc_selected_idx]\n",
    "\n",
    "    # Indices of cells (in the cell array) mapped to each nucleoid. These are not the Segment IDs\n",
    "    cell_index_nuc = first_cell[nuc_selected_idx]\n",
    "\n",
    "    # ONLY NO MAPPED NUCLEOIDS: Make repeated ones -1\n",
    "    # Make cell parquet index \"-1\"\n",
    "    cell_parquet_ids[cell_index_nuc == -1] = -1\n",
    "    \n",
    "    return cell_index_nuc, cells_per_nuc, nuc_parquet_ids, cell_parquet_ids\n",
    "    \n",
    "def reconcile_trenches(ref_df,masked_df,key=\"Trenchid Timepoint Index\"):\n",
    "    trench_id_tpt_indices_1 = set(ref_df[key].unique().tolist())\n",
    "    trench_id_tpt_indices_2 = set(masked_df[key].unique().tolist())\n",
    "    overlap = sorted(list(trench_id_tpt_indices_1&trench_id_tpt_indices_2))\n",
    "    masked_df_mask = masked_df[key].isin(overlap)\n",
    "    masked_df = masked_df[masked_df_mask]\n",
    "    return masked_df\n",
    "\n",
    "def assign_nuc_stats_to_cells(nuc_df):\n",
    "    \n",
    "    grouped_nuc_df = nuc_df.groupby('Cell Parquet Index', sort = False)\n",
    "    \n",
    "    # Number of nucleoids\n",
    "    nuc_number = grouped_nuc_df.apply(len)\n",
    "    nuc_number.name = 'Number of Nucleoids'\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    nuc_data_per_cell = pd.DataFrame(nuc_number)\n",
    "\n",
    "    # Compute Total Nuc area, Maj Axis length and Nucleoid separations Per Cell Segment\n",
    "    nuc_data_per_cell = (nuc_data_per_cell\n",
    "                         .join(grouped_nuc_df['area'].sum()) # Total Nucleoid Area\n",
    "                         .rename(columns={'area': 'Total Nucleoid Area'})\n",
    "                         .join(grouped_nuc_df['axis_major_length'].sum()) # Major axis length\n",
    "                         .rename(columns={'axis_major_length': 'Total Nucleoid Length'})\n",
    "                         .join(grouped_nuc_df['centroid_y'].apply(np.diff)) # Nucleoid separations\n",
    "                         .rename(columns={'centroid_y': 'Nucleoid Separations'})\n",
    "                        )\n",
    "    \n",
    "    # Get separation stats\n",
    "    nuc_data_per_cell['Nucleoid Separation Mean'] = nuc_data_per_cell['Nucleoid Separations'].apply(np.mean)\n",
    "    nuc_data_per_cell['Nucleoid Separation CV'] = nuc_data_per_cell['Nucleoid Separations'].apply(np.std)/nuc_data_per_cell['Nucleoid Separation Mean']\n",
    "    nuc_data_per_cell['Nucleoid Separation CV'] = nuc_data_per_cell['Nucleoid Separation CV'].replace({0:np.nan})\n",
    "    nuc_data_per_cell['Nucleoid Separation Min'] = nuc_data_per_cell['Nucleoid Separations'].apply(np.min, args=(None, None, False, np.inf))\n",
    "    nuc_data_per_cell['Nucleoid Separation Min'] = nuc_data_per_cell['Nucleoid Separation Min'].replace({np.inf:np.nan})\n",
    "    nuc_data_per_cell['Nucleoid Separation Max'] = nuc_data_per_cell['Nucleoid Separations'].apply(np.max, args=(None, None, False, -np.inf))\n",
    "    nuc_data_per_cell['Nucleoid Separation Max'] = nuc_data_per_cell['Nucleoid Separation Max'].replace({-np.inf:np.nan})\n",
    "    nuc_data_per_cell = nuc_data_per_cell.drop('Nucleoid Separations', axis=1) # Drop nucleoid separations\n",
    "\n",
    "    # Get rest of stats\n",
    "    mean_and_cv_properties = ['area', 'solidity', 'mCherry mean_intensity']\n",
    "    mean_and_cv_properties_names = ['Nucleoid Area', 'Nucleoid Solidity', 'Nucleoid Mean Intensity']\n",
    "\n",
    "    mean_only_properties = ['axis_major_length', 'axis_minor_length', 'eccentricity', 'orientation'] # 2023-04-04: Added Eccentricity and orientation\n",
    "    mean_only_properties_names = ['Nucleoid Major Axis Length', 'Nucleoid Minor Axis Length', 'Eccentricity', 'Orientation']\n",
    "\n",
    "    for i, metric in enumerate(mean_and_cv_properties):\n",
    "        # Get means\n",
    "        nuc_data_per_cell = (nuc_data_per_cell\n",
    "                             .join(grouped_nuc_df[metric].mean())\n",
    "                             .rename(columns={metric: mean_and_cv_properties_names[i] + ' Mean'})\n",
    "                            )\n",
    "\n",
    "        # Get CVs\n",
    "        cv_df = grouped_nuc_df[metric].std()/nuc_data_per_cell[mean_and_cv_properties_names[i] + ' Mean']\n",
    "        cv_df.name = mean_and_cv_properties_names[i] + ' CV'\n",
    "        nuc_data_per_cell =  nuc_data_per_cell.join(cv_df)\n",
    "        # nuc_data_per_cell.rename(columns={metric: mean_and_cv_properties_names[i] + ' CV'}, inplace=True)\n",
    "\n",
    "    for i, metric in enumerate(mean_only_properties):\n",
    "        nuc_data_per_cell = (nuc_data_per_cell\n",
    "                             .join(grouped_nuc_df[metric].mean())\n",
    "                             .rename(columns={metric: mean_only_properties_names[i] + ' Mean'})\n",
    "                            )\n",
    "    # nuc_data_per_cell.drop(-1, inplace=True) # Drop unassigned nucleoids\n",
    "    \n",
    "    # Merge with cell df\n",
    "    return nuc_data_per_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2a423-d322-4dc4-9200-31135c0e2177",
   "metadata": {},
   "source": [
    "## Merge Nucleoid Data to Lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b3f7cf4-c507-45cf-9883-0ef14c716ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "headpath_list = [\"/home/de64/scratch/de64/sync_folder/2023-03-25_lDE28_Run_1/Growth_Division\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44ddeda-e672-497a-91fa-d24db8efe3e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Dask dataframes\n",
    "\n",
    "for experiment_i, headpath in enumerate(headpath_list):\n",
    "    cell_lineage_path = headpath + \"/lineage\"\n",
    "    nucleoid_path = headpath + \"/analysis\"\n",
    "    lineage_output_path = headpath + \"/lineage_wNucleoid\"\n",
    "    nucleoid_output_path = headpath + \"/analysisNucsMappedToCells\"    \n",
    "    \n",
    "    yfp_lineages = dd.read_parquet(cell_lineage_path,engine='pyarrow',calculate_divisions=True)\n",
    "    mch_regprops = dd.read_parquet(nucleoid_path,engine='pyarrow',calculate_divisions=True)\n",
    "    \n",
    "    # Repartition to match File indices and reconcile trenches\n",
    "    file_indices_list = sorted(yfp_lineages[\"File Index\"].unique().compute().tolist())\n",
    "    file_indices_list = file_indices_list + [file_indices_list[-1] + 1]\n",
    "    \n",
    "    yfp_lineages = yfp_lineages.reset_index().set_index(\"File Index\",sorted=True).repartition(divisions=file_indices_list,force=True)\\\n",
    "                    .reset_index().set_index(\"File Parquet Index\",sorted=True).persist()\n",
    "    mch_regprops = mch_regprops.reset_index().set_index(\"File Index\",sorted=True).repartition(divisions=file_indices_list,force=True)\\\n",
    "                    .reset_index().set_index(\"File Parquet Index\",sorted=True).persist()\n",
    "    \n",
    "    wait(yfp_lineages);\n",
    "    wait(mch_regprops);\n",
    "    \n",
    "    mch_regprops= dd.map_partitions(reconcile_trenches,yfp_lineages,mch_regprops,transform_divisions=False,align_dataframes=False).persist()\n",
    "    yfp_lineages= dd.map_partitions(reconcile_trenches,mch_regprops,yfp_lineages,transform_divisions=False,align_dataframes=False).persist()\n",
    "    wait(mch_regprops);\n",
    "    wait(yfp_lineages);\n",
    "    \n",
    "    # Get meta df\n",
    "    empty_output = mch_regprops.get_partition(0).compute().loc[-1:-1]\n",
    "    empty_output[\"Segment Index\"] = []\n",
    "    empty_output[\"Segment Index\"] = empty_output[\"Segment Index\"].astype(int)\n",
    "    \n",
    "    empty_output[\"Cells Per Nucleoid\"] = []\n",
    "    empty_output[\"Cells Per Nucleoid\"] = empty_output[\"Cells Per Nucleoid\"].astype(int)\n",
    "    \n",
    "    empty_output[\"Nuc Parquet Index\"] = []\n",
    "    empty_output[\"Nuc Parquet Index\"] = empty_output[\"Nuc Parquet Index\"].astype('int64')\n",
    "    \n",
    "    empty_output[\"Cell Parquet Index\"] = []\n",
    "    empty_output[\"Cell Parquet Index\"] = empty_output[\"Cell Parquet Index\"].astype('int64')\n",
    "    \n",
    "    # Assign nucleoids to cells\n",
    "    mch_regprops = dd.map_partitions(assign_nucleoids_to_cells_df,mch_regprops,yfp_lineages,transform_divisions=False,align_dataframes=False,meta=empty_output).persist()\n",
    "    wait(mch_regprops);\n",
    "    \n",
    "    # Get Area, number of nucleoids per cell, and merge with cell df\n",
    "    # Generate meta df\n",
    "    empty_df = yfp_lineages.get_partition(0).compute().iloc[-1:-1]\n",
    "    cols_to_remove = empty_df.columns\n",
    "    empty_df = empty_df.reindex(columns=list(empty_df.columns)+\n",
    "           ['Number of Nucleoids', 'Total Nucleoid Area', 'Total Nucleoid Length',\n",
    "            'Nucleoid Separation Mean', 'Nucleoid Separation CV',\n",
    "            'Nucleoid Separation Min', 'Nucleoid Separation Max',\n",
    "            'Nucleoid Area Mean', 'Nucleoid Area CV', 'Nucleoid Solidity Mean',\n",
    "            'Nucleoid Solidity CV', 'Nucleoid Mean Intensity Mean',\n",
    "            'Nucleoid Mean Intensity CV', 'Nucleoid Major Axis Length Mean',\n",
    "            'Nucleoid Minor Axis Length Mean', 'Eccentricity Mean', 'Orientation Mean'])\n",
    "    empty_df=empty_df.drop(columns=cols_to_remove)\n",
    "    empty_df['Number of Nucleoids'] = empty_df['Number of Nucleoids'].astype('int64')   \n",
    "    \n",
    "    # Generate stats df\n",
    "    nuc_data_per_cell_dd = dd.map_partitions(assign_nuc_stats_to_cells,mch_regprops,transform_divisions=False,align_dataframes=False,meta=empty_df).persist()\n",
    "    wait(nuc_data_per_cell_dd);\n",
    "    \n",
    "    # Save mcherry df\n",
    "    dd.to_parquet(mch_regprops, nucleoid_output_path, engine=\"pyarrow\", overwrite=True)\n",
    "    del mch_regprops\n",
    "    \n",
    "    # Load again\n",
    "    yfp_lineages = dd.read_parquet(cell_lineage_path,engine='pyarrow',calculate_divisions=True)\n",
    "    \n",
    "    # Join to stats df\n",
    "    yfp_lineages = yfp_lineages.join(nuc_data_per_cell_dd, how = 'left')\n",
    "    \n",
    "    # wait(yfp_lineages);\n",
    "    yfp_lineages['Total Nucleoid Area'] = yfp_lineages['Total Nucleoid Area'].fillna(0)\n",
    "    yfp_lineages['Total Nucleoid Length'] = yfp_lineages['Total Nucleoid Length'].fillna(0)\n",
    "    yfp_lineages['Number of Nucleoids'] = yfp_lineages['Number of Nucleoids'].fillna(0)\n",
    "    yfp_lineages['Nucleoid Area Mean'] = yfp_lineages['Nucleoid Area Mean'].fillna(0)\n",
    "    yfp_lineages['Nucleoid Mean Intensity Mean'] = yfp_lineages['Nucleoid Mean Intensity Mean'].fillna(0)\n",
    "    yfp_lineages['Nucleoid Major Axis Length Mean'] = yfp_lineages['Nucleoid Major Axis Length Mean'].fillna(0)\n",
    "    yfp_lineages['Nucleoid Minor Axis Length Mean'] = yfp_lineages['Nucleoid Minor Axis Length Mean'].fillna(0)\n",
    "    # wait(yfp_lineages);\n",
    "    \n",
    "    # Compute variables involving both nuc and cell variables\n",
    "    yfp_lineages['Total Nucleoid Length per Cell Length'] = yfp_lineages['Total Nucleoid Length']/yfp_lineages['Length']\n",
    "    yfp_lineages['Total Nucleoid Area per Cell Area'] = yfp_lineages['Total Nucleoid Area']/yfp_lineages['area']\n",
    "    yfp_lineages['Total Number of Nucleoids per Cell Length'] = yfp_lineages['Number of Nucleoids']/yfp_lineages['Length']\n",
    "    \n",
    "    # Save\n",
    "    dd.to_parquet(yfp_lineages, lineage_output_path, engine=\"pyarrow\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4b957c2-4c14-4139-9a32-abc25b228d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 20:40:00,390 - distributed.deploy.adaptive_core - INFO - Adaptive stop\n"
     ]
    }
   ],
   "source": [
    "dask_controller.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
